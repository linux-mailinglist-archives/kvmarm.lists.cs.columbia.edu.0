Return-Path: <kvmarm-bounces@lists.cs.columbia.edu>
X-Original-To: lists+kvmarm@lfdr.de
Delivered-To: lists+kvmarm@lfdr.de
Received: from mm01.cs.columbia.edu (mm01.cs.columbia.edu [128.59.11.253])
	by mail.lfdr.de (Postfix) with ESMTP id E42C514788
	for <lists+kvmarm@lfdr.de>; Mon,  6 May 2019 11:21:06 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by mm01.cs.columbia.edu (Postfix) with ESMTP id 1C8EF4A321;
	Mon,  6 May 2019 05:21:06 -0400 (EDT)
X-Virus-Scanned: at lists.cs.columbia.edu
X-Spam-Flag: NO
X-Spam-Score: -4.201
X-Spam-Level: 
X-Spam-Status: No, score=-4.201 required=6.1 tests=[BAYES_00=-1.9,
	DNS_FROM_AHBL_RHSBL=2.699, RCVD_IN_DNSWL_HI=-5] autolearn=unavailable
Received: from mm01.cs.columbia.edu ([127.0.0.1])
	by localhost (mm01.cs.columbia.edu [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id J78FoZ-cpoi7; Mon,  6 May 2019 05:21:06 -0400 (EDT)
Received: from mm01.cs.columbia.edu (localhost [127.0.0.1])
	by mm01.cs.columbia.edu (Postfix) with ESMTP id C673D4A4A4;
	Mon,  6 May 2019 05:21:04 -0400 (EDT)
Received: from localhost (localhost [127.0.0.1])
 by mm01.cs.columbia.edu (Postfix) with ESMTP id D49F44A47E
 for <kvmarm@lists.cs.columbia.edu>; Mon,  6 May 2019 05:21:03 -0400 (EDT)
X-Virus-Scanned: at lists.cs.columbia.edu
Received: from mm01.cs.columbia.edu ([127.0.0.1])
 by localhost (mm01.cs.columbia.edu [127.0.0.1]) (amavisd-new, port 10024)
 with ESMTP id bEPcsJxHN-bG for <kvmarm@lists.cs.columbia.edu>;
 Mon,  6 May 2019 05:21:02 -0400 (EDT)
Received: from foss.arm.com (usa-sjc-mx-foss1.foss.arm.com [217.140.101.70])
 by mm01.cs.columbia.edu (Postfix) with ESMTP id 7F18C4A332
 for <kvmarm@lists.cs.columbia.edu>; Mon,  6 May 2019 05:21:02 -0400 (EDT)
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
 by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id CA479A78;
 Mon,  6 May 2019 02:21:01 -0700 (PDT)
Received: from big-swifty.misterjones.org (usa-sjc-mx-foss1.foss.arm.com
 [217.140.101.70])
 by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id 899323F5AF;
 Mon,  6 May 2019 02:21:00 -0700 (PDT)
Date: Mon, 06 May 2019 10:21:10 +0100
Message-ID: <86k1f49bw9.wl-marc.zyngier@arm.com>
From: Marc Zyngier <marc.zyngier@arm.com>
To: Heyi Guo <guoheyi@huawei.com>
Subject: Re: ARM/gic-v4: deadlock occurred
In-Reply-To: <db3bb9c1-8b3a-760d-057f-b8fb6914809b@huawei.com>
References: <9efe0260-4a84-7489-ecdd-2e9561599320@huawei.com>
 <86lfzl9ofe.wl-marc.zyngier@arm.com>
 <db3bb9c1-8b3a-760d-057f-b8fb6914809b@huawei.com>
User-Agent: Wanderlust/2.15.9 (Almost Unreal) SEMI-EPG/1.14.7 (Harue)
 FLIM/1.14.9 (=?UTF-8?B?R29qxY0=?=) APEL/10.8 EasyPG/1.0.0 Emacs/26
 (aarch64-unknown-linux-gnu) MULE/6.0 (HANACHIRUSATO)
Organization: ARM Ltd
MIME-Version: 1.0 (generated by SEMI-EPG 1.14.7 - "Harue")
Cc: linux-kernel@vger.kernel.org, kvmarm <kvmarm@lists.cs.columbia.edu>
X-BeenThere: kvmarm@lists.cs.columbia.edu
X-Mailman-Version: 2.1.14
Precedence: list
List-Id: Where KVM/ARM decisions are made <kvmarm.lists.cs.columbia.edu>
List-Unsubscribe: <https://lists.cs.columbia.edu/mailman/options/kvmarm>,
 <mailto:kvmarm-request@lists.cs.columbia.edu?subject=unsubscribe>
List-Archive: <https://lists.cs.columbia.edu/pipermail/kvmarm>
List-Post: <mailto:kvmarm@lists.cs.columbia.edu>
List-Help: <mailto:kvmarm-request@lists.cs.columbia.edu?subject=help>
List-Subscribe: <https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>,
 <mailto:kvmarm-request@lists.cs.columbia.edu?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Errors-To: kvmarm-bounces@lists.cs.columbia.edu
Sender: kvmarm-bounces@lists.cs.columbia.edu

On Sun, 05 May 2019 12:15:51 +0100,
Heyi Guo <guoheyi@huawei.com> wrote:
> 
> BTW since its_irq_set_vcpu_affinity() is already in atomic context,
> do we really need a separate lock its_dev->event_map.vlpi_lock? I
> didn't find anywhere outside its_irq_set_vcpu_affinity() call chain
> acquires this lock.

The reason is that the vlpi_maps array covers the whole of the
generating device, and not just a single interrupt. Relying on the
irq_desc lock to protect the array wouldn't work, as you could still
have concurrent accesses to the array (map, unmap and get all access
the same data).

So one way or another, we need some form of mutual exclusion at this
level. I guess one of the design mistakes that we have in the current
code is that there is no "device wide" operation, and that we rely on
map/unmap to perform the allocations on demand in the low level code.

What we could potentially do would be to move this allocation higher
up in the stack, and track the first time an LPI is turned into a VLPI
at that level. That's an invasive change though...

Thanks,

	M.

-- 
Jazz is not dead, it just smell funny.
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
https://lists.cs.columbia.edu/mailman/listinfo/kvmarm
