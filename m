Return-Path: <kvmarm-bounces@lists.cs.columbia.edu>
X-Original-To: lists+kvmarm@lfdr.de
Delivered-To: lists+kvmarm@lfdr.de
Received: from mm01.cs.columbia.edu (mm01.cs.columbia.edu [128.59.11.253])
	by mail.lfdr.de (Postfix) with ESMTP id B691E609F09
	for <lists+kvmarm@lfdr.de>; Mon, 24 Oct 2022 12:29:33 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by mm01.cs.columbia.edu (Postfix) with ESMTP id A05524B2AE;
	Mon, 24 Oct 2022 06:29:32 -0400 (EDT)
X-Virus-Scanned: at lists.cs.columbia.edu
X-Spam-Flag: NO
X-Spam-Score: -1.789
X-Spam-Level: 
X-Spam-Status: No, score=-1.789 required=6.1 tests=[BAYES_00=-1.9,
	DKIM_SIGNED=0.1, T_DKIM_INVALID=0.01, URIBL_BLOCKED=0.001]
	autolearn=unavailable
Authentication-Results: mm01.cs.columbia.edu (amavisd-new); dkim=softfail
	(fail, message has been altered) header.i=@kernel.org
Received: from mm01.cs.columbia.edu ([127.0.0.1])
	by localhost (mm01.cs.columbia.edu [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id 8j4rL8uGK9OO; Mon, 24 Oct 2022 06:29:32 -0400 (EDT)
Received: from mm01.cs.columbia.edu (localhost [127.0.0.1])
	by mm01.cs.columbia.edu (Postfix) with ESMTP id 33E584B253;
	Mon, 24 Oct 2022 06:29:31 -0400 (EDT)
Received: from localhost (localhost [127.0.0.1])
 by mm01.cs.columbia.edu (Postfix) with ESMTP id 071D549EC2
 for <kvmarm@lists.cs.columbia.edu>; Mon, 24 Oct 2022 06:29:30 -0400 (EDT)
X-Virus-Scanned: at lists.cs.columbia.edu
Received: from mm01.cs.columbia.edu ([127.0.0.1])
 by localhost (mm01.cs.columbia.edu [127.0.0.1]) (amavisd-new, port 10024)
 with ESMTP id O4dltteb1nVA for <kvmarm@lists.cs.columbia.edu>;
 Mon, 24 Oct 2022 06:29:28 -0400 (EDT)
Received: from ams.source.kernel.org (ams.source.kernel.org [145.40.68.75])
 by mm01.cs.columbia.edu (Postfix) with ESMTPS id 9D6FB49E18
 for <kvmarm@lists.cs.columbia.edu>; Mon, 24 Oct 2022 06:29:28 -0400 (EDT)
Received: from smtp.kernel.org (relay.kernel.org [52.25.139.140])
 (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
 (No client certificate requested)
 by ams.source.kernel.org (Postfix) with ESMTPS id 3F2A6B810B0;
 Mon, 24 Oct 2022 10:29:27 +0000 (UTC)
Received: by smtp.kernel.org (Postfix) with ESMTPSA id EE13AC433C1;
 Mon, 24 Oct 2022 10:29:25 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
 s=k20201202; t=1666607366;
 bh=e9EJjLUy6t6gwdKoVDL3bho6pQx55paAnahTf23c/+I=;
 h=Date:From:To:Cc:Subject:In-Reply-To:References:From;
 b=V5KSuSnkGIMXtpKaGL98PrEKJ9aQkHuz0r24lmwoCxFg6Ps/6RuaIyw8MPVoEEnJV
 WuQZE+qJyjyIoT8R000LLhkrk93xsxyMcLae7lKIcPUv6LS0WImMy3+FhzDHcLonpT
 WjSzZLadY4L+snHKyFblPA1QQ7uWkcpZqMA+P1vVxLfH7LhgD7cXvj9QuaGjycCjfX
 UTaJBGcG+0FHhgTMbdQ5PVINv5Y0ikRXakPxKYDFIzArgxjgiTokH9yQ9VT753eiwu
 DsVtouLX70chZw1YZ9h39HGP0i/5u/YEqsfbIPdx5/eCJb0kcgpo6kirMTosftJtTT
 NCBJBTiM3NyUw==
Received: from sofa.misterjones.org ([185.219.108.64]
 helo=goblin-girl.misterjones.org)
 by disco-boy.misterjones.org with esmtpsa (TLS1.3) tls
 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (Exim 4.95)
 (envelope-from <maz@kernel.org>) id 1omuhv-0017qA-Kt;
 Mon, 24 Oct 2022 11:29:23 +0100
Date: Mon, 24 Oct 2022 11:29:23 +0100
Message-ID: <86zgdlms58.wl-maz@kernel.org>
From: Marc Zyngier <maz@kernel.org>
To: Reiji Watanabe <reijiw@google.com>
Subject: Re: [PATCH 1/9] KVM: arm64: PMU: Align chained counter implementation
 with architecture pseudocode
In-Reply-To: <CAAeT=Fz55H09PWpmMu1sBkV=iUEHWezwhghJskaWAoqQsi2N0A@mail.gmail.com>
References: <20220805135813.2102034-1-maz@kernel.org>
 <20220805135813.2102034-2-maz@kernel.org>
 <CAAeT=Fz55H09PWpmMu1sBkV=iUEHWezwhghJskaWAoqQsi2N0A@mail.gmail.com>
User-Agent: Wanderlust/2.15.9 (Almost Unreal) SEMI-EPG/1.14.7 (Harue)
 FLIM-LB/1.14.9 (=?UTF-8?B?R29qxY0=?=) APEL-LB/10.8 EasyPG/1.0.0 Emacs/27.1
 (aarch64-unknown-linux-gnu) MULE/6.0 (HANACHIRUSATO)
MIME-Version: 1.0 (generated by SEMI-EPG 1.14.7 - "Harue")
X-SA-Exim-Connect-IP: 185.219.108.64
X-SA-Exim-Rcpt-To: reijiw@google.com, linux-arm-kernel@lists.infradead.org,
 kvmarm@lists.cs.columbia.edu, kvm@vger.kernel.org, kernel-team@android.com
X-SA-Exim-Mail-From: maz@kernel.org
X-SA-Exim-Scanned: No (on disco-boy.misterjones.org);
 SAEximRunCond expanded to false
Cc: kernel-team@android.com, kvmarm@lists.cs.columbia.edu,
 Linux ARM <linux-arm-kernel@lists.infradead.org>, kvm@vger.kernel.org
X-BeenThere: kvmarm@lists.cs.columbia.edu
X-Mailman-Version: 2.1.14
Precedence: list
List-Id: Where KVM/ARM decisions are made <kvmarm.lists.cs.columbia.edu>
List-Unsubscribe: <https://lists.cs.columbia.edu/mailman/options/kvmarm>,
 <mailto:kvmarm-request@lists.cs.columbia.edu?subject=unsubscribe>
List-Archive: <https://lists.cs.columbia.edu/pipermail/kvmarm>
List-Post: <mailto:kvmarm@lists.cs.columbia.edu>
List-Help: <mailto:kvmarm-request@lists.cs.columbia.edu?subject=help>
List-Subscribe: <https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>,
 <mailto:kvmarm-request@lists.cs.columbia.edu?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Errors-To: kvmarm-bounces@lists.cs.columbia.edu
Sender: kvmarm-bounces@lists.cs.columbia.edu

Hi Reiji,

Catching up on this.

On Tue, 23 Aug 2022 05:30:21 +0100,
Reiji Watanabe <reijiw@google.com> wrote:
> 
> Hi Marc,
> 
> On Fri, Aug 5, 2022 at 6:58 AM Marc Zyngier <maz@kernel.org> wrote:
> >
> > Ricardo recently pointed out that the PMU chained counter emulation
> > in KVM wasn't quite behaving like the one on actual hardware, in
> > the sense that a chained counter would expose an overflow on
> > both halves of a chained counter, while KVM would only expose the
> > overflow on the top half.
> >
> > The difference is subtle, but significant. What does the architecture
> > say (DDI0087 H.a):
> >
> > - Before PMUv3p4, all counters but the cycle counter are 32bit
> > - A 32bit counter that overflows generates a CHAIN event on the
> >   adjacent counter after exposing its own overflow status
> > - The CHAIN event is accounted if the counter is correctly
> >   configured (CHAIN event selected and counter enabled)
> >
> > This all means that our current implementation (which uses 64bit
> > perf events) prevents us from emulating this overflow on the lower half.
> >
> > How to fix this? By implementing the above, to the letter.
> >
> > This largly results in code deletion, removing the notions of
> > "counter pair", "chained counters", and "canonical counter".
> > The code is further restructured to make the CHAIN handling similar
> > to SWINC, as the two are now extremely similar in behaviour.
> >
> > Reported-by: Ricardo Koller <ricarkol@google.com>
> > Signed-off-by: Marc Zyngier <maz@kernel.org>
> > ---
> >  arch/arm64/kvm/pmu-emul.c | 324 +++++++++++---------------------------
> >  include/kvm/arm_pmu.h     |   2 -
> >  2 files changed, 91 insertions(+), 235 deletions(-)
> >
> > diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
> > index 11c43bed5f97..4986e8b3ea6c 100644
> > --- a/arch/arm64/kvm/pmu-emul.c
> > +++ b/arch/arm64/kvm/pmu-emul.c

[...]

> > +/*
> > + * Perform an increment on any of the counters described in @mask,
> > + * generating the overflow if required, and propagate it as a chained
> > + * event if possible.
> > + */
> > +static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
> > +                                     unsigned long mask, u32 event)
> > +{
> > +       int i;
> > +
> > +       if (!kvm_vcpu_has_pmu(vcpu))
> > +               return;
> > +
> > +       if (!(__vcpu_sys_reg(vcpu, PMCR_EL0) & ARMV8_PMU_PMCR_E))
> > +               return;
> > +
> > +       /* Weed out disabled counters */
> > +       mask &= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
> > +
> > +       for_each_set_bit(i, &mask, ARMV8_PMU_CYCLE_IDX) {
> > +               u64 type, reg;
> > +
> > +               /* Filter on event type */
> > +               type = __vcpu_sys_reg(vcpu, PMEVTYPER0_EL0 + i);
> > +               type &= kvm_pmu_event_mask(vcpu->kvm);
> > +               if (type != event)
> > +                       continue;
> > +
> > +               /* Increment this counter */
> > +               reg = __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) + 1;
> > +               reg = lower_32_bits(reg);
> > +               __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) = reg;
> > +
> > +               if (reg) /* No overflow? move on */
> > +                       continue;
> > +
> > +               /* Mark overflow */
> > +               __vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(i);
> 
> Perhaps it might be useful to create another helper that takes
> care of just one counter (it would essentially do the code above
> in the loop). The helper could be used (in addition to the above
> loop) from the code below for the CHAIN event case and from
> kvm_pmu_perf_overflow(). Then unnecessary execution of
> for_each_set_bit() could be avoided for these two cases.

I'm not sure it really helps. We would still need to check whether the
counter is enabled, and we'd need to bring that into the helper
instead of keeping it outside of the loop.

[...]

> > @@ -625,30 +528,27 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
> >  {
> >         struct arm_pmu *arm_pmu = vcpu->kvm->arch.arm_pmu;
> >         struct kvm_pmu *pmu = &vcpu->arch.pmu;
> > -       struct kvm_pmc *pmc;
> > +       struct kvm_pmc *pmc = &pmu->pmc[select_idx];
> >         struct perf_event *event;
> >         struct perf_event_attr attr;
> >         u64 eventsel, counter, reg, data;
> >
> > -       /*
> > -        * For chained counters the event type and filtering attributes are
> > -        * obtained from the low/even counter. We also use this counter to
> > -        * determine if the event is enabled/disabled.
> > -        */
> > -       pmc = kvm_pmu_get_canonical_pmc(&pmu->pmc[select_idx]);
> > -
> > -       reg = (pmc->idx == ARMV8_PMU_CYCLE_IDX)
> > +       reg = (select_idx == ARMV8_PMU_CYCLE_IDX)
> >               ? PMCCFILTR_EL0 : PMEVTYPER0_EL0 + pmc->idx;
> 
> You may want to use select_idx instead of pmc->id for consistency ?

Yes. Although Oliver had a point in saying that these pmc->idx vs
select_idx conversions were not strictly necessary and cluttered the
patch.

[...]

> > @@ -752,11 +607,15 @@ static void kvm_pmu_update_pmc_chained(struct kvm_vcpu *vcpu, u64 select_idx)
> >  void kvm_pmu_set_counter_event_type(struct kvm_vcpu *vcpu, u64 data,
> >                                     u64 select_idx)
> >  {
> > +       struct kvm_pmu *pmu = &vcpu->arch.pmu;
> > +       struct kvm_pmc *pmc = &pmu->pmc[select_idx];
> >         u64 reg, mask;
> >
> >         if (!kvm_vcpu_has_pmu(vcpu))
> >                 return;
> >
> > +       kvm_pmu_stop_counter(vcpu, pmc);
> 
> It appears that kvm_pmu_stop_counter() doesn't have to be called here
> because it is called in the beginning of kvm_pmu_create_perf_event().

It feels a bit odd to change the event type without stopping the
counter first, but I can't see anything going wrong if we omit it.

I'll drop it.

Thanks,

	M.

-- 
Without deviation from the norm, progress is not possible.
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
https://lists.cs.columbia.edu/mailman/listinfo/kvmarm
